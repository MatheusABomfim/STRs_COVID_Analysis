"""
STR annotation pipeline - Python version
"""

import os
import sys
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Set
from collections import defaultdict

import polars as pl
import pybedtools
import pysam
from pybedtools import BedTool

# ==============================
# SETUP
# ==============================

class Config:
    """ Pipeline settings"""
    # Input files - MODIFICADO: VCF → TSV
    TSV_PATH = "/home/matheusbomfim/projects/strs_paper/samples/gp_global/merged/str_consolidated.tsv"  # ALTERADO
    GTF_PATH = "/home/matheusbomfim/projects/strs_paper/gtf_annotation/Homo_sapiens.GRCh38.98.gtf"  # VERIFIQUE O CAMINHO
    GENOME_FILE = "/home/matheusbomfim/projects/strs_paper/gtf_annotation/genome.txt"  # VERIFIQUE O CAMINHO
    
    # Output directories
    OUTDIR = "../samples/gp_global/merged/"  
    
    # Parameters
    PROMOTER_DIST = 3000
    REQUIRE_STRAND_MATCH = False  # If True, use s=True in intersections
    MIN_INTRON_SIZE = 10  # Minimum size to consider an intron
    
    # Annotation priorities (lower number = higher priority)
    PRIORITY_MAP = {
        "CDS": 1,
        "five_prime_utr": 2,
        "three_prime_utr": 3,
        "promoter": 4,
        "intron": 5,
        "intergenic": 6,
        "others": 7
    }
    
    # Columns to use as identifier in BEDs
    # Can be "gene_id" (more precise) or "gene_name" (more readable)
    GENE_ID_COLUMN = "gene_id"
    
    # Temporary files control
    KEEP_TEMP_FILES = False

# Create output directory
Path(Config.OUTDIR).mkdir(parents=True, exist_ok=True)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="[%(asctime)s] %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(Path(Config.OUTDIR) / "annotation_pipeline.log")
    ]
)
log = logging.getLogger(__name__)

# ==============================
# FUNÇÕES AUXILIARES
# ==============================

def add_chr(chrom: str) -> str:
    """Adds 'chr' to chromosome if not present"""
    if chrom.startswith("chr"):
        return chrom
    return f"chr{chrom}"

def clean_temp_files():
    """Cleans temporary files if configured"""
    if not Config.KEEP_TEMP_FILES:
        temp_patterns = ["*.bed", "*.tmp"]
        for pattern in temp_patterns:
            for file in Path(Config.OUTDIR).glob(pattern):
                try:
                    file.unlink()
                    log.debug(f"Removed: {file}")
                except:
                    pass

def extract_gtf_attributes_optimized(df: pl.DataFrame) -> pl.DataFrame:
    # Extract gene_id and gene_name using vectorized regex
    df = df.with_columns([
        pl.col("attribute").str.extract(r'gene_id\s+"([^"]+)"', 1).alias("gene_id"),
        pl.col("attribute").str.extract(r'gene_name\s+"([^"]+)"', 1).alias("gene_name")
    ])
    
    # If gene_name is null, use gene_id as fallback
    df = df.with_columns(
        pl.when(pl.col("gene_name").is_null())
        .then(pl.col("gene_id"))
        .otherwise(pl.col("gene_name"))
        .alias("gene_name")
    )
    
    return df

def read_gtf_features(gtf_path: str) -> pl.DataFrame:
    """Reads GTF and returns optimized DataFrame"""
    
    log.info(f"Reading GTF: {gtf_path}")
    
    # Schema for performance
    schema = {
        "chrom": pl.Utf8,
        "source": pl.Utf8,
        "feature": pl.Utf8,
        "start": pl.Int64,
        "end": pl.Int64,
        "score": pl.Utf8,
        "strand": pl.Utf8,
        "frame": pl.Utf8,
        "attribute": pl.Utf8
    }
    
    try:
        df = pl.read_csv(
            gtf_path,
            separator="\t",
            has_header=False,
            comment_prefix="#",  # MUDOU: comment_char -> comment_prefix
            null_values=".",
            new_columns=list(schema.keys()),
            schema_overrides=schema  # MUDOU: dtypes -> schema_overrides
        )
    except Exception as e:
        log.error(f"Erro ao ler GTF: {e}")
        raise
    
    # Extract attributes
    df = extract_gtf_attributes_optimized(df)
    
    # Add 'chr' to chromosome
    df = df.with_columns(
        pl.col("chrom").map_elements(add_chr, return_dtype=pl.Utf8).alias("chrom")
    )
    
    # Select important columns
    df = df.select([
        "chrom", "start", "end", "feature", 
        "gene_id", "gene_name", "strand"
    ])
    
    log.info(f"GTF read: {df.shape[0]} rows, {df.shape[1]} columns")
    return df

def get_valid_chromosomes(genome_file: str) -> Set[str]:
    """Reads genome file and returns set of valid chromosomes"""
    valid_chroms = set()
    
    try:
        with open(genome_file, 'r') as f:
            for line in f:
                if line.strip():
                    chrom = line.split('\t')[0].strip()
                    valid_chroms.add(chrom)
        
        log.info(f"Valid chromosomes from genome file: {len(valid_chroms)}")
        log.debug(f"Chromosomes: {sorted(valid_chroms)}")
        
    except Exception as e:
        log.error(f"Error reading genome file: {e}")
        raise
    
    return valid_chroms


def filter_valid_chromosomes(df: pl.DataFrame, valid_chroms: Set[str]) -> pl.DataFrame:
    """Filters DataFrame to keep only valid chromosomes"""
    original_count = df.shape[0]
    
    # Filter
    df_filtered = df.filter(pl.col("chrom").is_in(list(valid_chroms)))
    
    filtered_count = df_filtered.shape[0]
    removed_count = original_count - filtered_count
    
    if removed_count > 0:
        # Get removed chromosomes
        removed_chroms = set(df.filter(~pl.col("chrom").is_in(list(valid_chroms)))["chrom"].unique().to_list())
        log.warning(f"Removed {removed_count} records from {len(removed_chroms)} invalid chromosomes: {sorted(removed_chroms)}")
    
    return df_filtered


def save_bedtool(df: pl.DataFrame, filename: str, 
                name_col: str = None,  # Se None, usa Config.GENE_ID_COLUMN
                score: int = 0) -> BedTool:
   
    path = Path(Config.OUTDIR) / filename
    
    # Use default column if not specified
    if name_col is None:
        name_col = Config.GENE_ID_COLUMN
    
    # Check if the column exists
    if name_col not in df.columns:
        log.warning(f"Coluna {name_col} não encontrada. Usando '.'")
        df = df.with_columns(pl.lit(".").alias("name"))
        name_col = "name"
    
    # Preparar DataFrame BED (6 colunas padrão)
    bed_df = df.select([
        "chrom", "start", "end",
        pl.col(name_col).alias("name"),
        pl.lit(score).alias("score"),
        "strand"
    ])
    
    # Save
    bed_df.write_csv(path, separator="\t", include_header=False)
    log.debug(f"Salvo: {path} ({bed_df.shape[0]} registros)")
    
    return BedTool(str(path))

def create_gene_regions(df_gtf: pl.DataFrame) -> Dict[str, BedTool]:
    log.info("Creating gene regions...")
    
    features = {}
    
    # 1. Genes
    genes = df_gtf.filter(pl.col("feature").str.to_lowercase() == "gene")
    features["gene"] = save_bedtool(genes, "genes.bed", name_col=Config.GENE_ID_COLUMN)
    
    # 2. Exons
    exons = df_gtf.filter(pl.col("feature").str.to_lowercase() == "exon")
    features["exon"] = save_bedtool(exons, "exons.bed", name_col=Config.GENE_ID_COLUMN)
    
    # 3. CDS
    cds = df_gtf.filter(pl.col("feature").str.to_lowercase() == "cds")
    features["CDS"] = save_bedtool(cds, "CDS.bed", name_col=Config.GENE_ID_COLUMN)
    
    # 4. UTRs
    utr5 = df_gtf.filter(pl.col("feature").str.to_lowercase() == "five_prime_utr")
    features["five_prime_utr"] = save_bedtool(utr5, "five_prime_utr.bed", name_col=Config.GENE_ID_COLUMN)
    
    utr3 = df_gtf.filter(pl.col("feature").str.to_lowercase() == "three_prime_utr")
    features["three_prime_utr"] = save_bedtool(utr3, "three_prime_utr.bed", name_col=Config.GENE_ID_COLUMN)
    
    return features

def create_promoters(genes_bed: BedTool) -> BedTool:

    log.info("Creating promoters...")
    
    # Create TSS (Transcription Start Site)
    tss_records = []
    for gene in genes_bed:
        if gene.strand == "+":
            tss_start = gene.start
            tss_end = tss_start + 1
        else:
            tss_end = gene.end
            tss_start = tss_end - 1
        
        tss_records.append([
            gene.chrom, tss_start, tss_end, 
            gene.name, 0, gene.strand
        ])
    
    # Create TSS BedTool
    if not tss_records:
        return BedTool("", from_string=True)
    
    tss_bed = BedTool("\n".join("\t".join(map(str, r)) for r in tss_records), 
                     from_string=True)
    
    # Expand to promoting region
    promoters = tss_bed.slop(
        g=Config.GENOME_FILE,
        l=Config.PROMOTER_DIST,
        r=0,
        s=True
    )
    
    # Save
    promoters.saveas(str(Path(Config.OUTDIR) / "promoters.bed"))
    log.info(f"Promotores criados: {len(promoters)}")
    return promoters

def create_introns_corrected(genes_bed: BedTool, exons_bed: BedTool) -> BedTool:
    """
    Creates introns CORRECTLY: groups exons by gene before subtracting
    Includes genes without exons as a single potential “intron”
    """
    log.info("Creating introns (corrected)...")
    
    # Group exons by gene (name/gene_id)
    exons_by_gene = defaultdict(list)
    for exon in exons_bed:
        gene_id = exon.name
        exons_by_gene[gene_id].append(exon)
    
    # Process each gene separately
    intron_records = []
    processed_genes = set()
    
    for gene in genes_bed:
        gene_id = gene.name
        processed_genes.add(gene_id)
        
        if gene_id in exons_by_gene:
            # Criar BedTool com éxons deste gene
            gene_exons = BedTool(exons_by_gene[gene_id]).sort()
            
            # Criar BedTool com o gene
            gene_bed = BedTool(f"{gene.chrom}\t{gene.start}\t{gene.end}\t{gene.name}\t0\t{gene.strand}", 
                             from_string=True)
            
            # Subtrair éxons para obter íntrons
            introns = gene_bed.subtract(gene_exons)
            
            for intron in introns:
                # Filtrar por tamanho mínimo
                if (intron.end - intron.start) >= Config.MIN_INTRON_SIZE:
                    intron_records.append([
                        intron.chrom, intron.start, intron.end,
                        gene_id, 0, gene.strand
                    ])
        else:
            # Gene sem éxons anotados - tratar como íntron único
            # (comum para genes não codificantes)
            if (gene.end - gene.start) >= Config.MIN_INTRON_SIZE:
                intron_records.append([
                    gene.chrom, gene.start, gene.end,
                    gene_id, 0, gene.strand
                ])
    
    # Create BedTool from introns
    if intron_records:
        introns_bed = BedTool("\n".join("\t".join(map(str, r)) for r in intron_records), 
                            from_string=True).sort()
    else:
        introns_bed = BedTool("", from_string=True)
    
    # Save
    introns_bed.saveas(str(Path(Config.OUTDIR) / "introns.bed"))
    log.info(f"Íntrons criados: {len(intron_records)} (genes processados: {len(processed_genes)})")
    return introns_bed

def create_intergenic_regions(genes_bed: BedTool) -> BedTool:

    log.info("Criando regiões intergênicas...")
    
    # Merge of genes
    genes_merged = genes_bed.sort().merge()
    
    # Supplement to obtain intergenic
    intergenic = genes_merged.complement(g=Config.GENOME_FILE)
    
    # Add metadata
    intergenic_records = []
    for region in intergenic:
        intergenic_records.append([
            region.chrom, region.start, region.end,
            "intergenic", 0, "."
        ])
    
    intergenic_bed = BedTool("\n".join("\t".join(map(str, r)) for r in intergenic_records),
                           from_string=True)
    
    # Save
    intergenic_bed.saveas(str(Path(Config.OUTDIR) / "intergenic.bed"))
    log.info(f"Regiões intergênicas criadas: {len(intergenic)}")
    return intergenic_bed

def process_tsv_to_bed(tsv_path: str) -> Tuple[BedTool, pl.DataFrame]:
    """Process TSV consolidated file instead of VCF"""
    log.info(f"Processing TSV: {tsv_path}")
    
    # Ler o arquivo TSV com Polars
    df_tsv = pl.read_csv(
        tsv_path,
        separator="\t",
        has_header=True
    )
    
    # Verificar se temos colunas necessárias
    required_cols = ["chrom", "start", "end", "repeat_unit"]
    for col in required_cols:
        if col not in df_tsv.columns:
            raise ValueError(f"Column {col} not found in TSV file")
    
    # Criar variantes únicas (combinação de chrom, start, end, repeat_unit)
    # Nota: a mesma posição pode aparecer em múltiplas amostras, mas queremos anotar uma vez
    df_variants = df_tsv.select(["chrom", "start", "end", "repeat_unit"]).unique()
    
    records = []
    for row in df_variants.iter_rows(named=True):
        chrom = add_chr(row["chrom"])
        start = row["start"] - 1  # Convert to 0-based for BED
        end = row["end"]  # BED uses 0-based start, 1-based end
        
        # Criar chave única
        key = f"{chrom}:{start+1}:{end}:{row['repeat_unit']}"
        
        records.append({
            "var_chrom": chrom,
            "var_start": start,
            "var_end": end,
            "key": key,
            "pos": row["start"],  # Original 1-based position
            "end": row["end"],
            "repeat_unit": row["repeat_unit"],
            "region_length": row["end"] - row["start"] + 1
        })
    
    # Criar DataFrame
    if records:
        df = pl.DataFrame(records)
    else:
        df = pl.DataFrame(schema=["var_chrom", "var_start", "var_end", "key", "pos", "end", "repeat_unit", "region_length"])
    
    # Criar BedTool
    if records:
        bed_str = "\n".join(
            f"{row['var_chrom']}\t{row['var_start']}\t{row['var_end']}\t{row['key']}"
            for row in records
        )
        bed = BedTool(bed_str, from_string=True).sort()
    else:
        bed = BedTool("", from_string=True)
    
    log.info(f"TSV processado: {df_tsv.shape[0]} linhas -> {df.shape[0]} variantes únicas")
    return bed, df

def intersect_with_hierarchy(strs_bed: BedTool, 
                           region_beds: Dict[str, BedTool]) -> pl.DataFrame:

    log.info("Performing intersections with hierarchy...")
    
    all_intersections = []
    
    for region_name, region_bed in region_beds.items():
        if region_bed.count() == 0:
            log.warning(f"Região vazia: {region_name}")
            continue
        
        log.info(f"  Intersectando com {region_name}...")
        
        # Configure intersection parameters
        intersect_kwargs = {
            "wa": True,
            "wb": True
        }
        if Config.REQUIRE_STRAND_MATCH:
            intersect_kwargs["s"] = True
        
        # Perform intersection
        intersected = strs_bed.intersect(region_bed.sort(), **intersect_kwargs)
        
        # Process results
        for interval in intersected:
            # interval.fields: [chr, start, end, key, chr, start, end, name, score, strand]
            str_key = interval.fields[3]
            gene_id = interval.fields[7]  # nome (gene_id)
            
            all_intersections.append({
                "key": str_key,
                "region": region_name,
                "gene_id": gene_id if gene_id != "." else None,
                "priority": Config.PRIORITY_MAP.get(region_name, 999)
            })
    
    # Convert to DataFrame
    if not all_intersections:
        return pl.DataFrame(schema=["key", "region", "gene_id", "priority"])
    
    df = pl.DataFrame(all_intersections)
    
    # Apply hierarchy (keep only the highest priority annotation)
    df = df.sort(["key", "priority"])
    df = df.unique(subset=["key"], keep="first")
    
    log.info(f"Interseções concluídas: {df.shape[0]} variantes anotadas")
    return df

def add_gene_information(df_annotations: pl.DataFrame, 
                        df_gtf: pl.DataFrame) -> pl.DataFrame:
    """Adds complete gene information to annotations"""
    
    log.info("Adding gene information...")
    
    # Create gene index (unique by gene_id)
    gene_index = df_gtf.filter(
        pl.col("feature").str.to_lowercase() == "gene"
    ).select([
        "gene_id", "gene_name", "chrom", "start", "end", "strand"
    ]).unique(subset=["gene_id"])
    
    # Rename columns for gene index
    gene_index = gene_index.rename({
        "chrom": "gene_chrom",
        "start": "gene_start",
        "end": "gene_end",
        "strand": "gene_strand"
    })
    
    # Merge with annotations using gene_id
    df_enriched = df_annotations.join(
        gene_index,
        on="gene_id",
        how="left"
    )
    
    # Fill nulls for variants without gene_id (intergenic, others)
    df_enriched = df_enriched.with_columns([
        pl.when(pl.col("gene_id").is_null())
        .then(pl.lit("."))
        .otherwise(pl.col("gene_id"))
        .alias("gene_id"),
        
        pl.when(pl.col("gene_name").is_null())
        .then(pl.lit("."))
        .otherwise(pl.col("gene_name"))
        .alias("gene_name"),
        
        pl.when(pl.col("gene_chrom").is_null())
        .then(pl.lit("."))
        .otherwise(pl.col("gene_chrom"))
        .alias("gene_chrom"),
        
        pl.when(pl.col("gene_start").is_null())
        .then(pl.lit(-1))
        .otherwise(pl.col("gene_start"))
        .alias("gene_start"),
        
        pl.when(pl.col("gene_end").is_null())
        .then(pl.lit(-1))
        .otherwise(pl.col("gene_end"))
        .alias("gene_end"),
        
        pl.when(pl.col("gene_strand").is_null())
        .then(pl.lit("."))
        .otherwise(pl.col("gene_strand"))
        .alias("gene_strand")
    ])
    
    # Create combined annotation field (region:gene_name or just region)
    df_enriched = df_enriched.with_columns(
        pl.when((pl.col("gene_name") == ".") | (pl.col("gene_name").is_null()))
        .then(pl.col("region"))
        .otherwise(pl.col("region") + ":" + pl.col("gene_name"))
        .alias("annotation")
    )
    
    return df_enriched


def annotate_others(df_strs: pl.DataFrame, df_annotated: pl.DataFrame) -> pl.DataFrame:
    log.info("Identifying unclassified variants...")
    
    # Obtain key sets
    all_keys = set(df_strs["key"].to_list())
    annotated_keys = set(df_annotated["key"].to_list())
    unannotated_keys = all_keys - annotated_keys
    
    log.info(f"  Total de variantes: {len(all_keys)}")
    log.info(f"  Anotadas: {len(annotated_keys)}")
    log.info(f"  Não anotadas (others): {len(unannotated_keys)}")
    
    # Create DataFrame for “others” variants
    if unannotated_keys:
        others_data = {
            "key": list(unannotated_keys),
            "region": ["others"] * len(unannotated_keys),
            "gene_id": ["."] * len(unannotated_keys),
            "priority": [Config.PRIORITY_MAP["others"]] * len(unannotated_keys)
        }
        
        df_others = pl.DataFrame(others_data)
        df_combined = pl.concat([df_annotated, df_others], how="vertical")
    else:
        df_combined = df_annotated
    
    return df_combined

def calculate_statistics(df_annotations: pl.DataFrame, 
                        df_strs: pl.DataFrame) -> Dict:
    """Calculates detailed annotation statistics"""
    
    log.info("Calculating statistics...")
    
    stats = {
        "summary": {},
        "distribution": {},
        "genes": {}
    }
    
    # Basic statistics
    stats["summary"]["total_variants"] = df_strs.shape[0]
    stats["summary"]["total_annotated"] = df_annotations.shape[0]
    
    # Distribution by region
    if "region" in df_annotations.columns:
        # value_counts() returns a DataFrame with columns: region, count
        region_counts_df = df_annotations["region"].value_counts()
        
        # Convert to dict properly
        region_counts = {}
        for row in region_counts_df.iter_rows(named=True):
            region_counts[row["region"]] = row["count"]
        
        stats["distribution"]["by_region"] = region_counts
        
        # Calculate percentages
        total = df_annotations.shape[0]
        if total > 0:
            stats["distribution"]["percentages"] = {
                region: (count / total) * 100
                for region, count in region_counts.items()
            }
    
    # Most affected genes (top 10)
    if "gene_name" in df_annotations.columns and "region" in df_annotations.columns:
        # Filter only variants with genes (exclude intergenic, others)
        gene_variants = df_annotations.filter(
            (pl.col("gene_name") != ".") & 
            (pl.col("region") != "intergenic") &
            (pl.col("region") != "others")
        )
        
        if gene_variants.shape[0] > 0:
            top_genes = (
                gene_variants.group_by(["gene_name", "gene_id"])
                .agg(pl.len().alias("variant_count"))
                .sort("variant_count", descending=True)
                .head(10)
                .to_dicts()
            )
            stats["genes"]["top_affected"] = top_genes
            
            # Unique genes with variants
            stats["genes"]["unique_genes_with_variants"] = gene_variants["gene_id"].n_unique()
    
    return stats

def save_results(df_final: pl.DataFrame, stats: Dict):
    log.info("Saving results...")
    
    # 1. Main annotated file
    output_file = Path(Config.OUTDIR) / "STRs_annotated_complete.tsv"
    df_final.write_csv(output_file, separator="\t")
    log.info(f"  Main annotations: {output_file}")
    
    # 2. Statistics
    stats_file = Path(Config.OUTDIR) / "annotation_statistics.tsv"
    
    stats_rows = []
    stats_rows.append({"category": "summary", "metric": "total_variants", "value": stats["summary"]["total_variants"]})
    stats_rows.append({"category": "summary", "metric": "total_annotated", "value": stats["summary"]["total_annotated"]})
    
    if "by_region" in stats["distribution"]:
        for region, count in stats["distribution"]["by_region"].items():
            percentage = stats["distribution"]["percentages"].get(region, 0)
            stats_rows.append({"category": "distribution", "metric": region, "value": count, "percentage": f"{percentage:.2f}"})
    
    stats_df = pl.DataFrame(stats_rows)
    stats_df.write_csv(stats_file, separator="\t")
    log.info(f"  Statistics: {stats_file}")
    
    # 3. Simplified version for analysis
    simple_file = Path(Config.OUTDIR) / "STRs_annotations_simple.tsv"
    simple_cols = ["var_chrom", "pos", "end", "repeat_unit", "region_length", "region", "gene_name"]
    if all(col in df_final.columns for col in simple_cols):
        df_final.select(simple_cols).write_csv(simple_file, separator="\t")
        log.info(f"  Simplified version: {simple_file}")
    
    # 4. Por gene
    if "gene_name" in df_final.columns and "region" in df_final.columns:
        gene_stats = (
            df_final.filter(pl.col("gene_name") != ".")
            .group_by(["gene_name", "gene_id", "region"])
            .agg(pl.count().alias("variant_count"))
            .sort(["gene_name", "variant_count"], descending=[False, True])
        )
        
        gene_file = Path(Config.OUTDIR) / "gene_statistics.tsv"
        gene_stats.write_csv(gene_file, separator="\t")
        log.info(f"  Gene statistics: {gene_file}")
        
def main():
    start_time = datetime.now()
    log.info("=" * 70)
    log.info("STARTING STR ANNOTATION PIPELINE FROM TSV")
    log.info("=" * 70)
    log.info(f"Configuration: gene_id_column={Config.GENE_ID_COLUMN}, "
             f"strand_match={Config.REQUIRE_STRAND_MATCH}")
    
    try:
        # ===== 1. READ AND PROCESS GTF =====
        df_gtf = read_gtf_features(Config.GTF_PATH)

        # ===== 1.5. FILTER VALID CHROMOSOMES =====
        valid_chroms = get_valid_chromosomes(Config.GENOME_FILE)
        df_gtf = filter_valid_chromosomes(df_gtf, valid_chroms)
        
        if df_gtf.shape[0] == 0:
            log.error("No valid chromosomes found after filtering!")
            return
        
        # ===== 2. CREATE BASIC GENE REGIONS =====
        basic_regions = create_gene_regions(df_gtf)
        
        # ===== 3. CREATE SPECIAL REGIONS =====
        log.info("Creating special regions...")
        
        # Promoters
        promoters = create_promoters(basic_regions["gene"])
        
        # Introns
        introns = create_introns_corrected(basic_regions["gene"], basic_regions["exon"])
        
        # Intergenics
        intergenic = create_intergenic_regions(basic_regions["gene"])
        
        # ===== 4. PROCESS TSV (em vez de VCF) =====
        strs_bed, df_strs = process_tsv_to_bed(Config.TSV_PATH)  # ALTERADO
        
        if strs_bed.count() == 0:
            log.error("No variants found in TSV!")
            return
        
        # ===== 5. PREPARE REGIONS FOR INTERSECTION =====
        all_regions = {
            "CDS": basic_regions["CDS"],
            "five_prime_utr": basic_regions["five_prime_utr"],
            "three_prime_utr": basic_regions["three_prime_utr"],
            "promoter": promoters,
            "intron": introns,
            "intergenic": intergenic
        }
        
        # ===== 6. PERFORM INTERSECTIONS WITH HIERARCHY =====
        df_annotated = intersect_with_hierarchy(strs_bed, all_regions)
        
        # ===== 7. IDENTIFY AND ADD "OTHERS" =====
        df_all_annotated = annotate_others(df_strs, df_annotated)
        
        # ===== 8. ADD GENE INFORMATION =====
        df_enriched = add_gene_information(df_all_annotated, df_gtf)
        
        # ===== 9. COMBINE WITH ORIGINAL TSV INFORMATION =====
        df_final = df_enriched.join(
            df_strs.select(["key", "var_chrom", "pos", "end", "repeat_unit", "region_length"]),
            on="key",
            how="left"
        )
        
        # Reorder columns for clarity
        final_columns = [
            "key", "var_chrom", "pos", "end", "repeat_unit", "region_length",
            "region", "annotation", "gene_name", "gene_id",
            "gene_chrom", "gene_start", "gene_end", "gene_strand",
            "priority"
        ]
        
        # Select only existing columns
        existing_cols = [col for col in final_columns if col in df_final.columns]
        df_final = df_final.select(existing_cols)
        
        # ===== 10. CALCULATE STATISTICS =====
        stats = calculate_statistics(df_all_annotated, df_strs)
        
        # ===== 11. SAVE RESULTS =====
        save_results(df_final, stats)
        
        # ===== 12. FINAL SUMMARY =====
        elapsed = datetime.now() - start_time
        log.info("=" * 70)
        log.info("PIPELINE SUCCESSFULLY COMPLETED!")
        log.info(f"Total time: {elapsed}")
        log.info(f"Unique STR variants processed: {stats['summary']['total_variants']}")
        log.info(f"Variants annotated: {stats['summary']['total_annotated']}")
        
        # Distribution log
        if "by_region" in stats["distribution"]:
            log.info("Distribution by region:")
            for region, count in stats["distribution"]["by_region"].items():
                percentage = stats["distribution"]["percentages"].get(region, 0)
                log.info(f"  {region}: {count} ({percentage:.1f}%)")
        
        log.info("=" * 70)
        
        # Cleaning temporary files
        clean_temp_files()
        
    except KeyboardInterrupt:
        log.warning("Pipeline interrupted by user")
        sys.exit(1)
    except Exception as e:
        log.error(f"Fatal error in pipeline: {e}", exc_info=True)
        sys.exit(1)

"""
STR annotation pipeline - Python version
"""

import os
import sys
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Set
from collections import defaultdict

import polars as pl
import pybedtools
import pysam
from pybedtools import BedTool

# ==============================
# SETUP
# ==============================

class Config:
    """ Pipeline settings"""
    # Input files - MODIFICADO: VCF → TSV
    TSV_PATH = "../samples/gp_global/merged/str_consolidated.tsv"
    GTF_PATH = "Homo_sapiens.GRCh38.98.gtf"
    GENOME_FILE = "genome.txt"

    # Output directories
    OUTDIR = "../samples/gp_global/merged/"

    # Parameters
    PROMOTER_DIST = 3000
    REQUIRE_STRAND_MATCH = True
    MIN_INTRON_SIZE = 0  # Minimum size to consider an intron

    # Annotation priorities (lower number = higher priority)
    PRIORITY_MAP = {
        "CDS": 1,
        "five_prime_utr": 2,
        "three_prime_utr": 3,
        "promoter": 4,
        "intron": 5,
        "intergenic": 6,
        "others": 7
    }

    # Columns to use as identifier in BEDs
    # Can be "gene_id" (more precise) or "gene_name" (more readable)
    GENE_ID_COLUMN = "gene_id"

    # Temporary files control
    KEEP_TEMP_FILES = False

# Create output directory
Path(Config.OUTDIR).mkdir(parents=True, exist_ok=True)

# ==============================
# Auxiliary functions
# ==============================

def add_chr(chrom: str) -> str:
    """Adds 'chr' to chromosome if not present"""
    if chrom.startswith("chr"):
        return chrom
    return f"chr{chrom}"

def clean_temp_files():
    """Cleans temporary files if configured"""
    if not Config.KEEP_TEMP_FILES:
        temp_patterns = ["*.bed", "*.tmp"]
        for pattern in temp_patterns:
            for file in Path(Config.OUTDIR).glob(pattern):
                try:
                    file.unlink()
                    print(f"Removed: {file}")
                except:
                    pass

def extract_gtf_attributes_optimized(df: pl.DataFrame) -> pl.DataFrame:
    # Extract gene_id and gene_name using vectorized regex
    df = df.with_columns([
        pl.col("attribute").str.extract(r'gene_id\s+"([^"]+)"', 1).alias("gene_id"),
        pl.col("attribute").str.extract(r'gene_name\s+"([^"]+)"', 1).alias("gene_name")
    ])

    # If gene_name is null, use fallback
    df = df.with_columns(
        pl.when(pl.col("gene_name").is_null())
        .then(pl.col("gene_id"))
        .otherwise(pl.col("gene_name"))
        .alias("gene_name")
    )

    return df

def read_gtf_features(gtf_path: str) -> pl.DataFrame:
    """Reads GTF and returns optimized DataFrame"""

    print(f"Reading GTF: {gtf_path}")

    # Schema for performance
    schema = {
        "chrom": pl.Utf8,
        "source": pl.Utf8,
        "feature": pl.Utf8,
        "start": pl.Int64,
        "end": pl.Int64,
        "score": pl.Utf8,
        "strand": pl.Utf8,
        "frame": pl.Utf8,
        "attribute": pl.Utf8
    }

    try:
        df = pl.read_csv(
            gtf_path,
            separator="\t",
            has_header=False,
            comment_prefix="#",  
            null_values=".",
            new_columns=list(schema.keys()),
            schema_overrides=schema  
        )
    except Exception as e:
        print(f"Error reading GTF: {e}")
        raise

    # Extract attributes
    df = extract_gtf_attributes_optimized(df)

    # Add 'chr' to chromosome
    df = df.with_columns(
        pl.col("chrom").map_elements(add_chr, return_dtype=pl.Utf8).alias("chrom")
    )

    # Select important columns
    df = df.select([
        "chrom", "start", "end", "feature",
        "gene_id", "gene_name", "strand"
    ])

    print(f"GTF read: {df.shape[0]} rows, {df.shape[1]} columns")
    return df

def get_valid_chromosomes(genome_file: str) -> Set[str]:
    """Reads genome file and returns set of valid chromosomes"""
    valid_chroms = set()

    try:
        with open(genome_file, 'r') as f:
            for line in f:
                if line.strip():
                    chrom = line.split('\t')[0].strip()
                    valid_chroms.add(chrom)

        print(f"Valid chromosomes from genome file: {len(valid_chroms)}")
        # print(f"Chromosomes: {sorted(valid_chroms)}")

    except Exception as e:
        print(f"Error reading genome file: {e}")
        raise

    return valid_chroms


def filter_valid_chromosomes(df: pl.DataFrame, valid_chroms: Set[str]) -> pl.DataFrame:
    """Filters DataFrame to keep only valid chromosomes"""
    original_count = df.shape[0]

    # Filter
    df_filtered = df.filter(pl.col("chrom").is_in(list(valid_chroms)))

    filtered_count = df_filtered.shape[0]
    removed_count = original_count - filtered_count

    if removed_count > 0:
        # Get removed chromosomes
        removed_chroms = set(df.filter(~pl.col("chrom").is_in(list(valid_chroms)))["chrom"].unique().to_list())
        print(f"Removed {removed_count} records from {len(removed_chroms)} invalid chromosomes: {sorted(removed_chroms)}")

    return df_filtered


def convert_gtf_to_bed_coords(df: pl.DataFrame) -> pl.DataFrame:
    """Convert GTF coordinates (1-based inclusive) to BED (0-based half-open)"""
    print(f"Converting GTF to BED coordinates: {df.shape[0]} records")
    return df.with_columns([
        (pl.col("start") - 1).alias("start"),  # 1-based → 0-based
        # end remains the same (BED end is exclusive, GTF end is inclusive)
    ])


def save_bedtool(df: pl.DataFrame, filename: str,
                name_col: str = None,
                score: int = 0,
                from_gtf: bool = True) -> BedTool:
    
    path = Path(Config.OUTDIR) / filename

    # Use default column if not specified
    if name_col is None:
        name_col = Config.GENE_ID_COLUMN

    # Check if the column exists
    if name_col not in df.columns:
        print(f"Column {name_col} not found. Using '.'")
        df = df.with_columns(pl.lit(".").alias("name"))
        name_col = "name"

    # Convert coordinates if from GTF
    if from_gtf:
        df = convert_gtf_to_bed_coords(df)
        print(f"Converted GTF to BED coordinates: {filename}")

    # Prepare BED DataFrame (6 standard columns)
    bed_df = df.select([
        "chrom", "start", "end",
        pl.col(name_col).alias("name"),
        pl.lit(score).alias("score"),
        "strand"
    ])

    # Save
    bed_df.write_csv(path, separator="\t", include_header=False)
    print(f"Saved: {path} ({bed_df.shape[0]} records)")

    return BedTool(str(path))

def create_gene_regions(df_gtf: pl.DataFrame) -> Dict[str, BedTool]:
    print("Creating gene regions...")

    features = {}

    # 1. Genes
    genes = df_gtf.filter(pl.col("feature").str.to_lowercase() == "gene")
    features["gene"] = save_bedtool(genes, "genes.bed", name_col=Config.GENE_ID_COLUMN, from_gtf=True)

    # 2. Exons
    exons = df_gtf.filter(pl.col("feature").str.to_lowercase() == "exon")
    features["exon"] = save_bedtool(exons, "exons.bed", name_col=Config.GENE_ID_COLUMN, from_gtf=True)

    # 3. CDS
    cds = df_gtf.filter(pl.col("feature").str.to_lowercase() == "cds")
    features["CDS"] = save_bedtool(cds, "CDS.bed", name_col=Config.GENE_ID_COLUMN, from_gtf=True)

    # 4. UTRs
    utr5 = df_gtf.filter(pl.col("feature").str.to_lowercase() == "five_prime_utr")
    features["five_prime_utr"] = save_bedtool(utr5, "five_prime_utr.bed", name_col=Config.GENE_ID_COLUMN, from_gtf=True)

    utr3 = df_gtf.filter(pl.col("feature").str.to_lowercase() == "three_prime_utr")
    features["three_prime_utr"] = save_bedtool(utr3, "three_prime_utr.bed", name_col=Config.GENE_ID_COLUMN, from_gtf=True)

    return features

def create_promoters(genes_bed: BedTool) -> BedTool:
    """Creates promoter regions upstream of TSS (0-based coordinates expected)"""
    print("Creating promoters (assuming gene BED is 0-based)...")

    # Create TSS (Transcription Start Site)
    tss_records = []
    for gene in genes_bed:
        # BED gene must already be in 0-based coordinates
        # For strand +: TSS = start (0-based)
        # For strand -: TSS = end - 1 (last base in 0-based)
        if gene.strand == "+":
            tss_start = gene.start  # 0-based TSS position
            tss_end = tss_start + 1
        else:
            # For negative strand, TSS is at end-1 (last base in 0-based)
            tss_start = gene.end - 1  # Last base in 0-based
            tss_end = tss_start + 1

        tss_records.append([
            gene.chrom, tss_start, tss_end,
            gene.name, 0, gene.strand
        ])

    # Create TSS BedTool
    if not tss_records:
        print("No TSS records created")
        return BedTool("", from_string=True)

    tss_bed = BedTool("\n".join("\t".join(map(str, r)) for r in tss_records),
                     from_string=True)

    # Expand to promoting region
    promoters = tss_bed.slop(
        g=Config.GENOME_FILE,
        l=Config.PROMOTER_DIST,
        r=0,
        s=True
    )

    # Save
    promoters.saveas(str(Path(Config.OUTDIR) / "promoters.bed"))
    print(f"Promoters created: {len(promoters)} (0-based coordinates)")
    return promoters

def create_introns_corrected(genes_bed: BedTool, exons_bed: BedTool) -> BedTool:
    """
    Creates introns CORRECTLY: groups exons by gene before subtracting
    Assumes genes_bed and exons_bed are already in 0-based coordinates
    """
    print("Creating introns (assuming gene/exon BEDs are 0-based)...")

    # Group exons by gene (name/gene_id)
    exons_by_gene = defaultdict(list)
    for exon in exons_bed:
        gene_id = exon.name
        exons_by_gene[gene_id].append(exon)

    # Process each gene separately
    intron_records = []
    processed_genes = set()

    for gene in genes_bed:
        gene_id = gene.name
        processed_genes.add(gene_id)

        if gene_id in exons_by_gene:
            # Create BedTool with exons of this gene (already 0-based)
            gene_exons = BedTool(exons_by_gene[gene_id]).sort()

            # Create BedTool with the gene (already 0-based)
            gene_bed = BedTool(f"{gene.chrom}\t{gene.start}\t{gene.end}\t{gene.name}\t0\t{gene.strand}",
                             from_string=True)

            # Subtract exons to obtain introns
            introns = gene_bed.subtract(gene_exons)

            for intron in introns:
                # Filter by minimum size (already 0-based)
                if (intron.end - intron.start) >= Config.MIN_INTRON_SIZE:
                    intron_records.append([
                        intron.chrom, intron.start, intron.end,
                        gene_id, 0, gene.strand
                    ])
        else:
            # Gene without annotated exons - treat as single intron
            # Filter by minimum size (already 0-based)
            if (gene.end - gene.start) >= Config.MIN_INTRON_SIZE:
                intron_records.append([
                    gene.chrom, gene.start, gene.end,
                    gene_id, 0, gene.strand
                ])

    # Create BedTool from introns
    if intron_records:
        introns_bed = BedTool("\n".join("\t".join(map(str, r)) for r in intron_records),
                            from_string=True).sort()
        print(f"Introns created: {len(intron_records)} (genes processed: {len(processed_genes)})")
    else:
        introns_bed = BedTool("", from_string=True)
        print("No introns created")

    # Save
    introns_bed.saveas(str(Path(Config.OUTDIR) / "introns.bed"))
    return introns_bed

def create_intergenic_regions(genes_bed: BedTool) -> BedTool:
    """Creates intergenic regions (0-based coordinates expected)"""
    print("Creating intergenic regions...")

    # Merge of genes
    genes_merged = genes_bed.sort().merge()

    # Supplement to obtain intergenic
    intergenic = genes_merged.complement(g=Config.GENOME_FILE)

    # Add metadata
    intergenic_records = []
    for region in intergenic:
        intergenic_records.append([
            region.chrom, region.start, region.end,
            "intergenic", 0, "."
        ])

    intergenic_bed = BedTool("\n".join("\t".join(map(str, r)) for r in intergenic_records),
                           from_string=True)

    # Save
    intergenic_bed.saveas(str(Path(Config.OUTDIR) / "intergenic.bed"))
    print(f"Intergenic regions created: {len(intergenic)}")
    return intergenic_bed

def process_tsv_to_bed(tsv_path: str) -> Tuple[BedTool, pl.DataFrame]:
    """Process TSV consolidated file instead of VCF"""
    print(f"Processing TSV: {tsv_path}")
    print("TSV is 1-based - converting to 0-based for BED format")

    # Read the TSV file with Polars
    df_tsv = pl.read_csv(
        tsv_path,
        separator="\t",
        has_header=True
    )

    # Check if required columns are present
    required_cols = ["chrom", "start", "end", "repeat_unit"]
    for col in required_cols:
        if col not in df_tsv.columns:
            raise ValueError(f"Column {col} not found in TSV file")

    # Create unique variants (combination of chrom, start, end, repeat_unit)
    df_variants = df_tsv.select(["chrom", "start", "end", "repeat_unit"]).unique()

    records = []
    for row in df_variants.iter_rows(named=True):
        chrom = add_chr(row["chrom"])
        
        # CONVERSION: 1-based → 0-based for BED
        # start 1-based: subtract 1 to become 0-based
        # end 1-based: remains the same because BED end is exclusive
        # Example: 1-based: [100, 200] → 0-based: [99, 200)
        start = row["start"] - 1  # Correct for 1-based → 0-based
        end = row["end"]          # BED end is exclusive, so keep as is
        
        # Create unique key keeping 1-based coordinate for readability
        key = f"{chrom}:{row['start']}:{row['end']}:{row['repeat_unit']}"

        records.append({
            "var_chrom": chrom,
            "var_start": start,    # 0-based (for BED)
            "var_end": end,        # 0-based exclusive (BED format)
            "key": key,
            "pos_1based": row["start"],  # Original 1-based for reference
            "end_1based": row["end"],    # Original 1-based for reference
            "repeat_unit": row["repeat_unit"],
            "region_length": row["end"] - row["start"] + 1  # Length 1-based
        })

    # Create DataFrame
    if records:
        df = pl.DataFrame(records)
    else:
        df = pl.DataFrame(schema=["var_chrom", "var_start", "var_end", "key", 
                                  "pos_1based", "end_1based", "repeat_unit", "region_length"])

    # Create BedTool
    if records:
        bed_str = "\n".join(
            f"{row['var_chrom']}\t{row['var_start']}\t{row['var_end']}\t{row['key']}"
            for row in records
        )
        bed = BedTool(bed_str, from_string=True).sort()
        print(f"Created BED with {bed.count()} STR regions (0-based)")
    else:
        bed = BedTool("", from_string=True)
        print("No STR regions found in TSV")

    print(f"TSV processed: {df_tsv.shape[0]} rows -> {df.shape[0]} unique variants")
    return bed, df

def intersect_with_hierarchy(strs_bed: BedTool,
                           region_beds: Dict[str, BedTool]) -> pl.DataFrame:
    """
    Perform intersections with hierarchy
    BED format: chrom, start, end, name, score, strand
    Intersect output fields: 
      [0-5]: STR BED fields
      [6-11]: Region BED fields
    """
    print("Performing intersections with hierarchy...")

    all_intersections = []

    for region_name, region_bed in region_beds.items():
        if region_bed.count() == 0:
            print(f"Empty region: {region_name}")
            continue

        print(f"  Intersecting with {region_name} ({region_bed.count()} features)...")

        # Configure intersection parameters
        intersect_kwargs = {
            "wa": True,
            "wb": True
        }
        if Config.REQUIRE_STRAND_MATCH:
            intersect_kwargs["s"] = True

        # Perform intersection
        intersected = strs_bed.intersect(region_bed.sort(), **intersect_kwargs)

        # Process results
        for interval in intersected:
            # interval.fields: [chr, start, end, key, score, strand, 
            #                   chr_region, start_region, end_region, name_region, score_region, strand_region]
            str_key = interval.fields[3]      # 4th field of STR BED
            gene_id = interval.fields[9]      # 4th field of region BED (index 6+3)
            
            # Check if gene_id is not "." and not empty
            if gene_id == "." or not gene_id:
                gene_id_val = None
            else:
                gene_id_val = gene_id

            all_intersections.append({
                "key": str_key,
                "region": region_name,
                "gene_id": gene_id_val,
                "priority": Config.PRIORITY_MAP.get(region_name, 999)
            })

    # Convert to DataFrame
    if not all_intersections:
        print("No intersections found")
        return pl.DataFrame(schema=["key", "region", "gene_id", "priority"])

    df = pl.DataFrame(all_intersections)

    # Apply hierarchy (keep only the highest priority annotation)
    df = df.sort(["key", "priority"])
    df = df.unique(subset=["key"], keep="first")

    print(f"Intersections completed: {df.shape[0]} annotated variants")
    return df

def add_gene_information(df_annotations: pl.DataFrame,
                        df_gtf: pl.DataFrame) -> pl.DataFrame:
    """Adds complete gene information to annotations"""

    print("Adding gene information...")

    # Create gene index (unique by gene_id) - keeping 1-based coordinates from GTF
    gene_index = df_gtf.filter(
        pl.col("feature").str.to_lowercase() == "gene"
    ).select([
        "gene_id", "gene_name", "chrom", "start", "end", "strand"
    ]).unique(subset=["gene_id"])

    # Check for duplicates
    if gene_index.shape[0] != gene_index["gene_id"].n_unique():
        print(f"Duplicates found in gene index: {gene_index.shape[0]} rows, {gene_index['gene_id'].n_unique()} unique")
        gene_index = gene_index.unique(subset=["gene_id"], keep="first")

    # Rename columns for gene index
    gene_index = gene_index.rename({
        "chrom": "gene_chrom",
        "start": "gene_start",
        "end": "gene_end",
        "strand": "gene_strand"
    })

    # Merge with annotations using gene_id
    df_enriched = df_annotations.join(
        gene_index,
        on="gene_id",
        how="left"
    )

    # Fill nulls for variants without gene_id (intergenic, others)
    df_enriched = df_enriched.with_columns([
        pl.when(pl.col("gene_id").is_null())
        .then(pl.lit("."))
        .otherwise(pl.col("gene_id"))
        .alias("gene_id"),

        pl.when(pl.col("gene_name").is_null())
        .then(pl.lit("."))
        .otherwise(pl.col("gene_name"))
        .alias("gene_name"),

        pl.when(pl.col("gene_chrom").is_null())
        .then(pl.lit("."))
        .otherwise(pl.col("gene_chrom"))
        .alias("gene_chrom"),

        pl.when(pl.col("gene_start").is_null())
        .then(pl.lit(-1))
        .otherwise(pl.col("gene_start"))
        .alias("gene_start"),

        pl.when(pl.col("gene_end").is_null())
        .then(pl.lit(-1))
        .otherwise(pl.col("gene_end"))
        .alias("gene_end"),

        pl.when(pl.col("gene_strand").is_null())
        .then(pl.lit("."))
        .otherwise(pl.col("gene_strand"))
        .alias("gene_strand")
    ])

    # Create combined annotation field (region:gene_name or just region)
    df_enriched = df_enriched.with_columns(
        pl.when((pl.col("gene_name") == ".") | (pl.col("gene_name").is_null()))
        .then(pl.col("region"))
        .otherwise(pl.col("region") + ":" + pl.col("gene_name"))
        .alias("annotation")
    )

    return df_enriched


def annotate_others(df_strs: pl.DataFrame, df_annotated: pl.DataFrame) -> pl.DataFrame:
    print("Identifying unclassified variants...")

    # Obtain key sets
    all_keys = set(df_strs["key"].to_list())
    annotated_keys = set(df_annotated["key"].to_list())
    unannotated_keys = all_keys - annotated_keys

    print(f"  Total variants: {len(all_keys)}")
    print(f"  Annotated: {len(annotated_keys)}")
    print(f"  Unannotated (others): {len(unannotated_keys)}")

    # Create DataFrame for "others" variants
    if unannotated_keys:
        others_data = {
            "key": list(unannotated_keys),
            "region": ["others"] * len(unannotated_keys),
            "gene_id": ["."] * len(unannotated_keys),
            "priority": [Config.PRIORITY_MAP["others"]] * len(unannotated_keys)
        }

        df_others = pl.DataFrame(others_data)
        df_combined = pl.concat([df_annotated, df_others], how="vertical")
    else:
        df_combined = df_annotated

    return df_combined

def calculate_statistics(df_annotations: pl.DataFrame,
                        df_strs: pl.DataFrame) -> Dict:
    """Calculates detailed annotation statistics"""

    print("Calculating statistics...")

    stats = {
        "summary": {},
        "distribution": {},
        "genes": {}
    }

    # Basic statistics
    stats["summary"]["total_variants"] = df_strs.shape[0]
    stats["summary"]["total_annotated"] = df_annotations.shape[0]

    # Distribution by region
    if "region" in df_annotations.columns:
        # value_counts() returns a DataFrame with columns: region, count
        region_counts_df = df_annotations["region"].value_counts()

        # Convert to dict properly
        region_counts = {}
        for row in region_counts_df.iter_rows(named=True):
            region_counts[row["region"]] = row["count"]

        stats["distribution"]["by_region"] = region_counts

        # Calculate percentages
        total = df_annotations.shape[0]
        if total > 0:
            stats["distribution"]["percentages"] = {
                region: (count / total) * 100
                for region, count in region_counts.items()
            }

    # Most affected genes (top 10)
    if "gene_name" in df_annotations.columns and "region" in df_annotations.columns:
        # Filter only variants with genes (exclude intergenic, others)
        gene_variants = df_annotations.filter(
            (pl.col("gene_name") != ".") &
            (pl.col("region") != "intergenic") &
            (pl.col("region") != "others")
        )

        if gene_variants.shape[0] > 0:
            top_genes = (
                gene_variants.group_by(["gene_name", "gene_id"])
                .agg(pl.len().alias("variant_count"))
                .sort("variant_count", descending=True)
                .head(10)
                .to_dicts()
            )
            stats["genes"]["top_affected"] = top_genes

            # Unique genes with variants
            stats["genes"]["unique_genes_with_variants"] = gene_variants["gene_id"].n_unique()

    return stats

def save_results(df_final: pl.DataFrame, stats: Dict):
    print("Saving results...")

    # 1. Main annotated file
    output_file = Path(Config.OUTDIR) / "STRs_annotated_complete.tsv"
    df_final.write_csv(output_file, separator="\t")
    print(f"  Main annotations: {output_file}")

    # 2. Statistics
    stats_file = Path(Config.OUTDIR) / "annotation_statistics.tsv"

    stats_rows = []
    stats_rows.append({"category": "summary", "metric": "total_variants", "value": stats["summary"]["total_variants"]})
    stats_rows.append({"category": "summary", "metric": "total_annotated", "value": stats["summary"]["total_annotated"]})

    if "by_region" in stats["distribution"]:
        for region, count in stats["distribution"]["by_region"].items():
            percentage = stats["distribution"]["percentages"].get(region, 0)
            stats_rows.append({"category": "distribution", "metric": region, "value": count, "percentage": f"{percentage:.2f}"})

    stats_df = pl.DataFrame(stats_rows)
    stats_df.write_csv(stats_file, separator="\t")
    print(f"  Statistics: {stats_file}")

    # 3. Simplified version for analysis
    simple_file = Path(Config.OUTDIR) / "STRs_annotations_simple.tsv"
    simple_cols = ["var_chrom", "pos", "end", "repeat_unit", "region_length", "region", "gene_name"]
    
    # Check available columns
    available_cols = [col for col in simple_cols if col in df_final.columns]
    if available_cols:
        df_final.select(available_cols).write_csv(simple_file, separator="\t")
        print(f"  Simplified version: {simple_file}")
    else:
        print("  Insufficient columns to create simplified version")

    # 4. Gene statistics
    if "gene_name" in df_final.columns and "region" in df_final.columns:
        gene_stats = (
            df_final.filter(pl.col("gene_name") != ".")
            .group_by(["gene_name", "gene_id", "region"])
            .agg(pl.count().alias("variant_count"))
            .sort(["gene_name", "variant_count"], descending=[False, True])
        )

        gene_file = Path(Config.OUTDIR) / "gene_statistics.tsv"
        gene_stats.write_csv(gene_file, separator="\t")
        print(f"  Gene statistics: {gene_file}")

def validate_coordinate_conversion():
    """Validate coordinate conversion"""
    print("Coordinate conversion validation:")
    print("  Input TSV format: 1-based inclusive")
    print("  GTF format: 1-based inclusive")
    print("  BEDTools format: 0-based half-open")
    print("  Conversion: 1-based → 0-based (subtract 1 from start)")
    print("  Example: 1-based [100, 200] → 0-based [99, 200)")
    print("  Note: All BED files will be in 0-based coordinates")

def main():
    start_time = datetime.now()
    print("=" * 70)
    print("STARTING STR ANNOTATION PIPELINE FROM TSV")
    print("=" * 70)
    print(f"Configuration: gene_id_column={Config.GENE_ID_COLUMN}, "
          f"strand_match={Config.REQUIRE_STRAND_MATCH}")
    
    # Validate coordinate system
    validate_coordinate_conversion()

    try:
        # ===== 1. READ AND PROCESS GTF =====
        print("\n[Step 1] Reading GTF file...")
        df_gtf = read_gtf_features(Config.GTF_PATH)

        # ===== 1.5. FILTER VALID CHROMOSOMES =====
        print("\n[Step 1.5] Filtering valid chromosomes...")
        valid_chroms = get_valid_chromosomes(Config.GENOME_FILE)
        df_gtf = filter_valid_chromosomes(df_gtf, valid_chroms)

        if df_gtf.shape[0] == 0:
            print("ERROR: No valid chromosomes found after filtering!")
            return

        # ===== 2. CREATE BASIC GENE REGIONS =====
        print("\n[Step 2] Creating gene regions...")
        basic_regions = create_gene_regions(df_gtf)

        # ===== 3. CREATE SPECIAL REGIONS =====
        print("\n[Step 3] Creating special regions...")

        # Promoters
        print("  Creating promoters...")
        promoters = create_promoters(basic_regions["gene"])

        # Introns
        print("  Creating introns...")
        introns = create_introns_corrected(basic_regions["gene"], basic_regions["exon"])

        # Intergenics
        print("  Creating intergenic regions...")
        intergenic = create_intergenic_regions(basic_regions["gene"])

        # ===== 4. PROCESS TSV (convert 1-based to 0-based) =====
        print("\n[Step 4] Processing TSV file...")
        strs_bed, df_strs = process_tsv_to_bed(Config.TSV_PATH)

        if strs_bed.count() == 0:
            print("ERROR: No variants found in TSV!")
            return

        print(f"Processed {strs_bed.count()} STR regions (0-based)")
        print(f"All BED files now in 0-based coordinates")

        # ===== 5. PREPARE REGIONS FOR INTERSECTION =====
        print("\n[Step 5] Preparing regions for intersection...")
        all_regions = {
            "CDS": basic_regions["CDS"],
            "five_prime_utr": basic_regions["five_prime_utr"],
            "three_prime_utr": basic_regions["three_prime_utr"],
            "promoter": promoters,
            "intron": introns,
            "intergenic": intergenic
        }

        # ===== 6. PERFORM INTERSECTIONS WITH HIERARCHY =====
        print("\n[Step 6] Performing intersections...")
        df_annotated = intersect_with_hierarchy(strs_bed, all_regions)

        # ===== 7. IDENTIFY AND ADD "OTHERS" =====
        print("\n[Step 7] Identifying unannotated variants...")
        df_all_annotated = annotate_others(df_strs, df_annotated)

        # ===== 8. ADD GENE INFORMATION =====
        print("\n[Step 8] Adding gene information...")
        df_enriched = add_gene_information(df_all_annotated, df_gtf)

        # ===== 9. COMBINE WITH ORIGINAL TSV INFORMATION =====
        print("\n[Step 9] Combining with original TSV data...")
        # Verify unique keys
        if df_enriched["key"].n_unique() != df_enriched.shape[0]:
            print(f"WARNING: Multiple annotations per STR detected: {df_enriched.shape[0]} annotations for {df_enriched['key'].n_unique()} unique STRs")
            print("Using first annotation per STR (highest priority)")
        
        # Safe join
        df_final = df_enriched.join(
            df_strs.select(["key", "var_chrom", "pos_1based", "end_1based", "repeat_unit", "region_length"]),
            on="key",
            how="left"
        )

        # Rename columns for clarity
        df_final = df_final.rename({
            "pos_1based": "pos",
            "end_1based": "end"
        })

        # Reorder columns for clarity
        final_columns = [
            "key", "var_chrom", "pos", "end", "repeat_unit", "region_length",
            "region", "annotation", "gene_name", "gene_id",
            "gene_chrom", "gene_start", "gene_end", "gene_strand",
            "priority"
        ]

        # Select only existing columns
        existing_cols = [col for col in final_columns if col in df_final.columns]
        df_final = df_final.select(existing_cols)

        # ===== 10. CALCULATE STATISTICS =====
        print("\n[Step 10] Calculating statistics...")
        stats = calculate_statistics(df_all_annotated, df_strs)

        # ===== 11. SAVE RESULTS =====
        print("\n[Step 11] Saving results...")
        save_results(df_final, stats)

        # ===== 12. FINAL SUMMARY =====
        elapsed = datetime.now() - start_time
        print("\n" + "=" * 70)
        print("PIPELINE SUCCESSFULLY COMPLETED!")
        print(f"Total time: {elapsed}")
        print(f"Unique STR variants processed: {stats['summary']['total_variants']}")
        print(f"Variants annotated: {stats['summary']['total_annotated']}")

        # Distribution log
        if "by_region" in stats["distribution"]:
            print("Distribution by region:")
            for region, count in stats["distribution"]["by_region"].items():
                percentage = stats["distribution"]["percentages"].get(region, 0)
                print(f"  {region}: {count} ({percentage:.1f}%)")

        print("=" * 70)

        # Cleaning temporary files
        if not Config.KEEP_TEMP_FILES:
            print("Cleaning temporary files...")
            clean_temp_files()

    except KeyboardInterrupt:
        print("\nPipeline interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\nFatal error in pipeline: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
